{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liwenliang/anaconda3/envs/rob/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# listing embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_str = pickle.load(open('./datasets/flat_events.txt', 'rb'))\n",
    "train = events_str[:int(len(events_str)*0.9)] \n",
    "test = events_str[int(len(events_str)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2vec = Word2Vec(train, \n",
    "                    vector_size=50, \n",
    "                    min_count=1, \n",
    "                    sg=1, \n",
    "                    hs=1, \n",
    "                    negative=0, \n",
    "                    epochs=12,\n",
    "                    workers=1, \n",
    "                    seed = 0,\n",
    "                    window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of char dictionary: 263\n"
     ]
    }
   ],
   "source": [
    "print('size of char dictionary:', len(train2vec.wv.key_to_index)) #0.75:251 0.9:263 1:293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2vec.wv.save(\"./word_embed.bin\")\n",
    "train2vec.wv.save_word2vec_format(\"./word_embed.own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_index_train = np.zeros((382, 50)) \n",
    "with open('./word_embed.own', 'r') as f: \n",
    "    for i in f:\n",
    "        values = i.strip('\\n').split(' ')\n",
    "        if len(values) == 2 or values[0] == '': \n",
    "            continue  \n",
    "        word = int(values[0])\n",
    "        embedding = np.asarray(values[1:], dtype='float32')                                                           \n",
    "        word_embeddings_index_train[word] = embedding\n",
    "        word_embeddings_index_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# item feature embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_info = pd.read_csv('./datasets/item_info.csv',' ')\n",
    "id_dic = item_info.set_index('item_id')['item_vec'].to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_class1,item_feature_class2,item_feature_class3,item_feature_reg = np.zeros((382, 1)) ,np.zeros((382, 1)),np.zeros((382, 1)),np.zeros((382, 2)) \n",
    "for key,value in id_dic.items():\n",
    "    value =value.split(',')\n",
    "    item_feature_class1[key] = np.asarray(value[0], dtype='int64')\n",
    "    item_feature_class2[key] = np.asarray(value[1], dtype='int64')\n",
    "    item_feature_class3[key] = np.asarray(value[2], dtype='int64')\n",
    "    item_feature_reg[key] = np.asarray(value[3:], dtype='float32')\n",
    "item_feature = np.concatenate([item_feature_class1,item_feature_class2,item_feature_class3,item_feature_reg],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# item embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 55)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to properly range item_vec to listing embedding space\n",
    "item_embedding = np.concatenate((word_embeddings_index_train,item_feature),axis=1)\n",
    "np.shape(item_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(item_embedding, open('item_embedding.txt','wb'),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature1: User-item Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_score(n_user,exposed_items_v,item_embedding,events_v):\n",
    "    #calculate the score of item_embedding and user_embedding\n",
    "    exposed_item_embedding_v = np.zeros((n_user,9,50))\n",
    "    user_item_embedding_v = np.zeros((n_user, 50))\n",
    "    emb_v =  np.zeros((n_user, exposed_item_embedding_v.shape[1]+2))#######+2\n",
    "    for n,i in enumerate(exposed_items_v):\n",
    "        item_list = i[:].split(',')\n",
    "        for m,j in enumerate(item_list):\n",
    "            exposed_item_embedding_v[n,m,:] = item_embedding[int(j)][:50]\n",
    "    item_event_v = events_v['item_event'].tolist()\n",
    "    time_weights_v = events_v['time_weights'].tolist()\n",
    "    \n",
    "    for n,i in enumerate(item_event_v):\n",
    "        item_list = i[1:-1].split(',')\n",
    "        time_list = time_weights_v[n][1:-1].split(',')\n",
    "        if item_list[0]=='0':\n",
    "            user_item_embedding_v[n] += item_embedding[0][:50]*0.6\n",
    "        else:\n",
    "            for k,j in enumerate(item_list):\n",
    "                user_item_embedding_v[n] += item_embedding[int(j)][:50]*float(time_list[k])\n",
    "        user_item_embedding_v[n]/=len(item_list)\n",
    "        \n",
    "    for i in range(n_user):\n",
    "        emb_v[i,:9] = user_item_embedding_v[i,:].dot(exposed_item_embedding_v[i,:].transpose())\n",
    "    #     emb_v[i,:9] = user_item_embedding_v[i].dot(exposed_item_embedding_v[i].transpose().dot(events_v['interval'][i]))\n",
    "    emb_v[:,9] = (emb_v[:,0]+emb_v[:,1]+emb_v[:,2])/3\n",
    "    emb_v[:,10] = (emb_v[:,3]+emb_v[:,4]+emb_v[:,5])/3\n",
    "    c_v = pd.DataFrame(emb_v, columns= ['scores']*9+['group_scores']*2)#####\n",
    "    return c_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature2: Item Protrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_protrait(n_user,events_v,exposed_items_v,item_vec_dic):\n",
    "    item_protrait_v = np.zeros((n_user,9,4))\n",
    "    for n,i in enumerate(exposed_items_v):\n",
    "        item_list = i[:].split(',')\n",
    "        for m,j in enumerate(item_list):\n",
    "            item_protrait_v[n,m,:] = item_vec_dic[int(j)-1][[0,1,3,4]]\n",
    "    item_protrait_v = item_protrait_v.reshape([n_user,-1])\n",
    "    a_v = pd.DataFrame(item_protrait_v,columns = ['item_class','item_class','rare','probability']*9)\n",
    "    return a_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature3: User Protrait\n",
    "user_protrait的各列类别数：[**3**, 1363, **20**, **10**, 195, 49,**3**, **11**, **2**, 2164]（除了1、9都作为类别处理试试）\n",
    "\n",
    "直接相加类别和为3820，实际总类别为3570，1、4、5、6、9中有重复元素\n",
    "\n",
    "检查测试集是否与训练集类别一致就可以决定是否能当类别：[3*,1319,19,10,191,47,3*,13,2,2054]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_protrait(events_v):    \n",
    "    user_protrait_v = events_v['user_protrait'].tolist()\n",
    "    user_protrait_v =np.array([i[:].split(',')for i in user_protrait_v] ).astype('int64')\n",
    "    b_v = pd.DataFrame(user_protrait_v,columns = ['user']*10)\n",
    "    b_v['user']=b_v['user'].astype('category')\n",
    "    b_v.loc[:,'interval'] = events_v['interval']\n",
    "    return b_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature4: Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(n_user,item_info,exposed_items_v):\n",
    "    item_price = item_info['price'].tolist()\n",
    "    p_v = np.zeros((n_user,9))\n",
    "    for n,i in enumerate(exposed_items_v):\n",
    "        item_list = i[:].split(',')\n",
    "        for m,j in enumerate(item_list):\n",
    "            p_v[n,m] = item_price[int(j)-1]\n",
    "    p_v = pd.DataFrame(p_v,columns = ['price']*9)\n",
    "    return p_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature5: User Class Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_class_distance(n_user,item_vec_dic,exposed_items_v,events_v):\n",
    "    item_event_v = events_v['item_event'].tolist()\n",
    "    time_weights_v = events_v['time_weights'].tolist()\n",
    "    user_item_class=np.zeros((n_user, 2))#price\\rare\n",
    "    for n,i in enumerate(item_event_v):\n",
    "        item_list = i[1:-1].split(',')\n",
    "        time_list = time_weights_v[n][1:-1].split(',')\n",
    "        if not item_list[0]=='0':\n",
    "            for k,j in enumerate(item_list):\n",
    "                user_item_class[n][:] += item_vec_dic[int(j)-1][:2]#*float(time_list[k])\n",
    "        user_item_class[n]/=len(item_list)\n",
    "\n",
    "    exposed_item_class = np.zeros((n_user,9,2))\n",
    "    for n,i in enumerate(exposed_items_v):\n",
    "        item_list = i[:].split(',')\n",
    "        for m,j in enumerate(item_list):\n",
    "            exposed_item_class[n,m,:] = item_vec_dic[int(j)-1][:2]\n",
    "    exposed_item_class.shape\n",
    "    cla = np.zeros((n_user, exposed_item_class.shape[1]))\n",
    "    for n,user in enumerate(exposed_item_class):\n",
    "        if user_item_class[n][0]:\n",
    "            for k,item in enumerate(user):\n",
    "                cla[n][k]=cosine(user_item_class[n][0],item[0])\n",
    "        else:cla[n] = [1.0]*9#试一下倒数\n",
    "    C_v  =pd.DataFrame(1/cla, columns= ['distance']*9)\n",
    "    return C_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证集\n",
    "def mydataset(event_name,data_name):\n",
    "    events_v = pd.read_csv('./datasets/'+event_name+'.csv')\n",
    "    item_info = pd.read_csv('./datasets/item_info.csv',' ')\n",
    "    item_embedding = pickle.load(open('item_embedding.txt', 'rb'))\n",
    "    exposed_items_v = events_v['exposed_items'].tolist()\n",
    "    item_vec = item_info['item_vec'].tolist()\n",
    "    item_vec_dic =np.array([i[:].split(',')for i in item_vec] ).astype('float64')\n",
    "    n_user = len(exposed_items_v)\n",
    "    c_v = user_item_score(n_user,exposed_items_v,item_embedding,events_v)\n",
    "    C_v = user_class_distance(n_user,item_vec_dic,exposed_items_v,events_v)\n",
    "    a_v = item_protrait(n_user,events_v,exposed_items_v,item_vec_dic)\n",
    "    b_v = user_protrait(events_v)\n",
    "    p_v = price(n_user,item_info,exposed_items_v)\n",
    "    data_i_v = pd.concat([c_v,C_v,a_v,b_v,p_v],axis = 1)\n",
    "    data_i_v.to_csv('./datasets/'+data_name+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liwenliang/anaconda3/envs/rob/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mydataset('event','data')\n",
    "mydataset('event_v','data_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rob",
   "language": "python",
   "name": "rob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
